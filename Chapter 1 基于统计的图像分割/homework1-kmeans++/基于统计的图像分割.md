# 基于统计的图像分割

主要思路：

通过图片的rgb属性，将所有像素点作为一堆samples。每个样本具有三个属性（Red，Green，Blue），在此基础上也可以使用像素点在图片上的位置信息作为特征，即（Red，Green，Blue，$\lambda x$，$\lambda y$），通过$\lambda$参数可以调节位置信息在分割中的重要性。



## Kmeans ++ 初始化方法

引用：https://www.cnblogs.com/pinard/p/6164214.html

K-Means++的对于初始化质心的优化策略如下：

1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心𝜇1μ1

2. 对于数据集中的每一个点$x_i$，计算它与已选择的聚类中心中最近聚类中心的距离
   $$
   D(x_i) = arg \min || x_i - \mu_r||_2^2, r = 1, 2, ..., k_{selected}
   $$

3. 选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x_i)$较大的点，被选取作为聚类中心的概率较大

4. 重复2和3直到选择出k个聚类质心

5. 利用这k个质心来作为初始化质心去运行标准的K-Means算法



## Kmeans ++ 初始化代码

```c++
// kmeans ++
    int init_random_sample_index = rand() % samples_.size();
    centers_[0].feature_ = samples_[init_random_sample_index].feature_;

    // 一次迭代用于选择一个中心
    for (int now_cluster = 1; now_cluster < centers_.size(); now_cluster ++)
    {
        // 计算所有sample到聚类中心最近距离
        for (int now_sample = 0; now_sample < samples_.size(); now_sample ++)
        {
            // 初始化距离
            samples_[now_sample].distance_ = calc_square_distance(
                centers_[0].feature_, samples_[now_sample].feature_);

            // 计算它与已选择的聚类中心中最近聚类中心的距离
            for (int check_k = 1; check_k < now_cluster; check_k ++)
            {
                float now_distance = calc_square_distance(
                    centers_[check_k].feature_, samples_[now_sample].feature_);
                if (samples_[now_sample].distance_ > now_distance)
                    samples_[now_sample].distance_ = now_distance;
            }
        }
        
        // 依据距离概率采样
        float array_sum = 0, now_sum = 0, next_sum = samples_[0].distance_;
        for (int i = 0; i < samples_.size(); i ++)
        {
            array_sum += samples_[i].distance_;
        }

        srand((int)time(NULL)); // 设置随机数种子

        float now_random_num = rand() / array_sum;

        int sample_result;

        for (int i = 0; i < samples_.size() - 1; i ++)
        {
            if (now_sum < now_random_num && next_sum > now_random_num)
            {
                sample_result = i;
                break;
            }
            now_sum += samples_[i].distance_;
            next_sum += samples_[i + 1].distance_;
        }

        centers_[now_cluster].feature_ = samples_[sample_result].feature_;
    }
}
```



## Kmeans实现代码（随机初始化）

```c++
#include "k_means.h"
#include <algorithm>
#include <vector>

// to generate random number
static std::random_device rd;
static std::mt19937 rng(rd());

/**
 * @brief get_random_index, check_convergence, calc_square_distance are helper
 * functions, you can use it to finish your homework:)
 *
 */

std::set<int> get_random_index(int max_idx, int n);

float check_convergence(const std::vector<Center>& current_centers,
                        const std::vector<Center>& last_centers);

inline float calc_square_distance(const std::array<float, 3>& arr1,
                                  const std::array<float, 3>& arr2);

/**
 * @brief Construct a new Kmeans object
 *
 * @param img : image with 3 channels
 * @param k : wanted number of cluster
 */
Kmeans::Kmeans(cv::Mat img, const int k) {
    centers_.resize(k);
    last_centers_.resize(k);
    samples_.reserve(img.rows * img.cols);

    // save each feature vector into samples
    for (int r = 0; r < img.rows; r++) {
        for (int c = 0; c < img.cols; c++) {
            std::array<float, 3> tmp_feature;
            for (int channel = 0; channel < 3; channel++) {
                tmp_feature[channel] =
                    static_cast<float>(img.at<cv::Vec3b>(r, c)[channel]);
            }
            samples_.emplace_back(tmp_feature, r, c, -1);
        }
    }
}

/**
 * @brief initialize k centers randomly, using set to ensure there are no
 * repeated elements
 *
 */
// TODO Try to implement a better initialization function
void Kmeans::initialize_centers() {
    std::set<int> random_idx =
        get_random_index(samples_.size() - 1, centers_.size());
    int i_center = 0;

    for (auto index : random_idx) {
        centers_[i_center].feature_ = samples_[index].feature_;
        i_center++;
    }
}

/**
 * @brief change the label of each sample to the nearst center
 *
 */
void Kmeans::update_labels() {
    for (Sample& sample : samples_) {
        // TODO update labels of each feature
        float now = 999999999.0;
        for (int i = 0; i < centers_.size(); i ++) {
            float temp = calc_square_distance(sample.feature_, centers_[i].feature_);
            if (temp < now)
            {
                now = temp;
                sample.label_ = i;
            }
        }
    }
}

/**
 * @brief move the centers according to new lables
 *
 */
void Kmeans::update_centers() {
    // backup centers of last iteration
    last_centers_ = centers_;
    // calculate the mean value of feature vectors in each cluster
    // TODO complete update centers functions.
    for (int i = 0; i < centers_.size(); i ++)
    {
        float temp_r = 0, temp_g = 0, temp_b = 0;
        int temp_count = 0;
        for (Sample& sample : samples_)
        {
            if (sample.label_ == i)
            {
                temp_count ++;
                temp_r += sample.feature_[0];
                temp_g += sample.feature_[1];
                temp_b += sample.feature_[2];
            }
        }

        centers_[i].feature_[0] = temp_r / temp_count;
        centers_[i].feature_[1] = temp_g / temp_count;
        centers_[i].feature_[2] = temp_b / temp_count;
    }
}

/**
 * @brief check terminate conditions, namely maximal iteration is reached or it
 * convergents
 *
 * @param current_iter
 * @param max_iteration
 * @param smallest_convergence_radius
 * @return true
 * @return false
 */
bool Kmeans::is_terminate(int current_iter, int max_iteration,
                          float smallest_convergence_radius) const {
    // TODO Write a terminate function.
    // helper funtion: check_convergence(const std::vector<Center>&
    // current_centers, const std::vector<Center>& last_centers)
    if (current_iter >= max_iteration) return true;

    if (check_convergence(centers_, last_centers_) <= smallest_convergence_radius)
        return true;

    return false;
}

std::vector<Sample> Kmeans::get_result_samples() const {
    return samples_;
}
std::vector<Center> Kmeans::get_result_centers() const {
    return centers_;
}
/**
 * @brief Execute k means algorithm
 *                1. initialize k centers randomly
 *                2. assign each feature to the corresponding centers
 *                3. calculate new centers
 *                4. check terminate condition, if it is not fulfilled, return
 *                   to step 2
 * @param max_iteration
 * @param smallest_convergence_radius
 */
void Kmeans::run(int max_iteration, float smallest_convergence_radius) {
    initialize_centers();
    int current_iter = 0;
    while (!is_terminate(current_iter, max_iteration,
                         smallest_convergence_radius)) {
        current_iter++;
        update_labels();
        update_centers();
    }
}

/**
 * @brief Get n random numbers from 1 to parameter max_idx
 *
 * @param max_idx
 * @param n
 * @return std::set<int> A set of random numbers, which has n elements
 */
std::set<int> get_random_index(int max_idx, int n) {
    std::uniform_int_distribution<int> dist(1, max_idx + 1);

    std::set<int> random_idx;
    while (random_idx.size() < n) {
        random_idx.insert(dist(rng) - 1);
    }
    return random_idx;
}
/**
 * @brief Calculate the L2 norm of current centers and last centers
 *
 * @param current_centers current assigned centers with 3 channels
 * @param last_centers  last assigned centers with 3 channels
 * @return float
 */
float check_convergence(const std::vector<Center>& current_centers,
                        const std::vector<Center>& last_centers) {
    float convergence_radius = 0;
    for (int i_center = 0; i_center < current_centers.size(); i_center++) {
        convergence_radius +=
            calc_square_distance(current_centers[i_center].feature_,
                                 last_centers[i_center].feature_);
    }
    return convergence_radius;
}

/**
 * @brief calculate L2 norm of two arrays
 *
 * @param arr1
 * @param arr2
 * @return float
 */
inline float calc_square_distance(const std::array<float, 3>& arr1,
                                  const std::array<float, 3>& arr2) {
    return std::pow((arr1[0] - arr2[0]), 2) + std::pow((arr1[1] - arr2[1]), 2) +
           std::pow((arr1[2] - arr2[2]), 2);
}
```



## 结果（随机初始化）

![image-20211205004726960](https://s2.loli.net/2021/12/05/MOeE9Qc1rxPboip.png)

k = 5

![image-20211205004759530](https://s2.loli.net/2021/12/05/ojdvDQ7BPJ8nFtY.png)

k = 5

![image-20211205004837037](https://s2.loli.net/2021/12/05/tzkPXug1HhI6rxj.png)

k = 3



## 结果（Kmeans++）

![image-20211205004142082](https://s2.loli.net/2021/12/05/FS7a9ZzsuPfm2QX.png)

k = 3

![image-20211205004233026](https://s2.loli.net/2021/12/05/DGYORQ1oqSrpcga.png)

k = 5

![image-20211205004302392](https://s2.loli.net/2021/12/05/VfDOG3n5Z8KY47E.png)

k = 5





## 关于聚类个数

随着聚类个数的增加，总体损失函数必然下降。



证明：

假如当前有$k$类，当类增加到$k+1$时，分为两种情况：

1. 至少存在一个点，与在$k$类时cluster中心点的距离大于第$k+1$个中心，此时它被分到这个类中，使得整个损失函数变小。
2. 不存在1中的点，即此时新的一类中没有任何一个点，在迭代中，这个新的中心会变更，直到变成1的情况。



总体损失函数与类别数目一般会为下图的形状，即在4之前，sui z和类别的增多，损失函数下降很快。但是4以后，损失函数虽然在下降，但是下降的很慢，因此我们可以使用4作为类别数目。

![image-20211205010034649](https://s2.loli.net/2021/12/05/OrSKRZHsj5o6tYn.png)
